{"cells":[{"cell_type":"markdown","metadata":{"id":"yv-6GI21Wa7l"},"source":["# README\n","\n","Minimum notebook that allows you to run the demo locally. Automatically loads pretrained SVM and NN from google drive so that you do not need to train them.\n","\n","Note: net_9d_scaling seems to be more resistant to hands that are far from the camera.\n","\n","Note: demo now runs a neural network. Demo can be switched to run SVM by changing the TODOs.\n","\n","Note: Only run this file on your local computer\n","\n","This file is described as the game playing program in our paper"]},{"cell_type":"markdown","metadata":{"id":"2EgFyrIQbVs6"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z9dW0FoIniG"},"outputs":[],"source":["%pip install gdown -q\n","# %pip install datasets\n","%pip install mediapipe -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZS9lU7XSWPc"},"outputs":[],"source":["# from datasets import load_dataset\n","#mediapipe dependencies\n","import mediapipe as mp\n","\n","#general dependencies\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","from PIL import Image\n","import shutil\n","import os\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"QEWtjNo_X2Sf"},"source":["# Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgjzoH1pX5ld"},"outputs":[],"source":["# Stringify a numeric label into textual form\n","def decode_label(label):\n","    if label == 1:\n","        return \"rock\"\n","    elif label == 0:\n","        return \"paper\"\n","    elif label == 2:\n","        return \"scissors\"\n","    elif label == 3:\n","        return \"none\"\n","    return None\n","    # return \"rock\" if label == 1 else \"paper\" if label == 0 else \"scissors\"\n","\n","# grey scale images (channel 1)\n","# def grey_scale():\n","#   transform = transforms.Compose([\n","#       transforms.Resize(256),\n","#       transforms.Grayscale(num_output_channels=1),\n","#   ])\n","#   return transform\n","\n","# returns a transformed image of shape (300, 300, 3)\n","def rgb_image_transform(image): # input type PIL.PngImagePlugin.PngImageFile\n","  resized_image = image.resize((300, 300))\n","  if resized_image.mode != 'RGB':\n","    resized_image = resized_image.convert('RGB')\n","\n","  return resized_image\n","\n","# returns a numpy (tensor) of an image\n","def image_numpy(image):\n","  np_image = np.array(image)\n","\n","  return np_image\n","\n","#Utility from Henry\n","from scipy.spatial.distance import euclidean\n","import numpy as np\n","import re\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","def parse_landmarks(landmarks_str):\n","    matches = re.findall(r'landmark \\{\\s*x: ([e\\d.-]+)\\s*y: ([e\\d.-]+)\\s*z: ([e\\d.-]+)\\s*\\}', landmarks_str)\n","    landmarks = [(float(x), float(y), float(z)) for x, y, z in matches]\n","    return landmarks\n","\n","def calculate_distances(landmarks):\n","    wrist = landmarks[0]\n","    fingertips_indexes = [4, 8, 12, 16]\n","    distances = [euclidean(wrist, landmarks[i]) for i in fingertips_indexes]\n","    return distances\n","\n","def calculate_9d(landmarks):\n","    wrist = landmarks[0]\n","    fingertips_indexes = [4, 8, 12, 16, 20, 5, 9, 13, 17]\n","    distances = [euclidean(wrist, landmarks[i]) for i in fingertips_indexes]\n","    return distances"]},{"cell_type":"markdown","metadata":{"id":"p9ApSqCSDZWm"},"source":["# Start Live Demo using SVM of Joint as the model (only in Jupyter notebook)"]},{"cell_type":"markdown","metadata":{"id":"_dp4syKTDlKQ"},"source":["### load the models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LaT8TRp5Dgmm"},"outputs":[],"source":["mp_drawing = mp.solutions.drawing_utils  # used to draw landmarks\n","mp_hands = mp.solutions.hands  # used to get landmarks from a photo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M01rDvuFWa7v"},"outputs":[],"source":["# download svm_9d.pkl\n","# !gdown --fuzzy https://drive.google.com/file/d/1-8hm-quWlYfQeXkMeR_lhkMxqV-d-Nkq/view?usp=drive_link\n","!gdown 1AD0EYalbRzWK86uJkHv1QQCuCHHWxuu7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exiaUbHlWa7v"},"outputs":[],"source":["from sklearn.svm import SVC\n","with open(\"svm.pkl\", 'rb') as f:\n","    svm_model = pickle.load(f)"]},{"cell_type":"markdown","source":["Note: the following loads NNs"],"metadata":{"id":"TdftW1zpWkMw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8hcmQNXWa7v"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Net(nn.Module):\n","    def __init__(self, input_features=5, hidden_layers=64):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(input_features, hidden_layers)  # Input layer\n","        self.drop1 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(hidden_layers, hidden_layers)\n","        self.drop2 = nn.Dropout(0.5)\n","        self.fc3 = nn.Linear(hidden_layers, 3)  # Output layer\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))  # Activation function for the input layer\n","        # x = self.drop1(x)\n","        x = torch.relu(self.fc2(x))  # No activation function for the output layer\n","        # x = self.drop2(x)\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","source":["!gdown 16IJoblVT0gGrbZLU6stAyIvhG5EDR9Zm"],"metadata":{"id":"GKqydrWoWtGp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"091E4ToOWa7v"},"outputs":[],"source":["net = Net(input_features=9, hidden_layers=64)\n","net.load_state_dict(torch.load(\"net_9d_scaling.pt\"))"]},{"cell_type":"markdown","metadata":{"id":"SQY5Xc37Dodq"},"source":["### start live feed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6qXOiXWWa7v"},"outputs":[],"source":["import cv2\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR_izseNWa7v"},"outputs":[],"source":["# run this to use nn\n","\n","def predict_model_nn(net, input):\n","    return decode_label(np.argmax(net(torch.tensor(input).float()).detach().numpy()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEytz-ocWa7w"},"outputs":[],"source":["# run this to use svm\n","\n","def predict_model(model, input):\n","    return decode_label(model.predict(np.array([input]))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJA4dQsWDqAK"},"outputs":[],"source":["\n","\n","def get_label(index, hand, results):\n","    output = None\n","    for idx, classification in enumerate(results.multi_handedness):\n","        if classification.classification[0].index == index:\n","            label = classification.classification[0].label\n","            score = classification.classification[0].score\n","            text = '{} {}'.format(label, round(score, 2))\n","\n","            # print(str(results.multi_hand_landmarks))\n","            landmarks = parse_landmarks(str(hand))\n","\n","            # TODO CHANGE THIS to use 9d\n","            # distance = calculate_distances(landmarks)\n","            distance = calculate_9d(landmarks)\n","            sum_distance = 0\n","            if isinstance(distance, list):\n","                sum_distance = np.sum(distance)\n","\n","            # TODO CHANGE THIS to use svm\n","            # type = predict_model(svm_model, distance)\n","            type = predict_model_nn(net, distance)\n","\n","            coords = tuple(np.multiply(np.array((hand.landmark[mp_hands.HandLandmark.WRIST].x, hand.landmark[mp_hands.HandLandmark.WRIST].y)),\n","            [640, 480]).astype(int))\n","\n","            output = text, coords, type, sum_distance\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHo-U_tsDt01"},"outputs":[],"source":["# rgb_image_transform\n","cap = cv2.VideoCapture(0)\n","\n","if not cap.isOpened():\n","    print(\"Error: Unable to open webcam\")\n","    exit()\n","\n","with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n","    while cap.isOpened():\n","        ret, frame = cap.read() # ret : return value, frame: the image frame from webcam\n","\n","        # BGR to RGB\n","        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        image = cv2.flip(image, 1)\n","\n","        image.flags.writeable = False\n","\n","        results = hands.process(image)\n","\n","        image.flags.writeable = True\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","        # print(results)\n","        # Detection\n","        if results.multi_hand_landmarks:\n","            first_hand = ''\n","            second_hand = ''\n","            winner = ''\n","            for num, hand in enumerate(results.multi_hand_landmarks):\n","                # Choose a different color for each landmark\n","                color = (0, 255, 0)  # Default color\n","                if num == 0:\n","                    color = (255, 0, 0)  # Change color for the first landmark\n","                elif num == 1:\n","                    color = (0, 0, 255)  # Change color for the second landmark\n","                # Draw the landmark\n","                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n","                                       mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=4),\n","                                       mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2))\n","\n","\n","                # render left or right detection\n","                if get_label(num, hand, results): # type : rock, paper, scissor\n","                    text, coord, type, sum = get_label(num, hand, results)\n","                    text_with_type = f'{text} - {type}'\n","                    cv2.putText(image, text_with_type, coord, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n","                    if (num == 0): # first hand\n","                        first_hand = type\n","                    else:\n","                        second_hand = type\n","\n","                if first_hand == second_hand:\n","                    winner = \"It's a tie!\"\n","                else:\n","                    if (first_hand == \"rock\" and second_hand == \"scissor\") or \\\n","                       (first_hand == \"scissor\" and second_hand == \"paper\") or \\\n","                       (first_hand == \"paper\" and second_hand == \"rock\"):\n","                        winner = first_hand\n","                    else:\n","                        winner = second_hand\n","\n","                image_height, image_width, _ = image.shape\n","                cv2.putText(image, winner, (int(image_width / 2) - 20, image_height - 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","\n","        cv2.imshow('Joint detection', image)\n","\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KdLOGr5Wa7w"},"outputs":[],"source":["# Run this cell if any windows linger\n","cap.release()\n","cv2.destroyAllWindows()"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["2EgFyrIQbVs6"],"gpuType":"V28","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}